defaults:
  - base

# See: https://arxiv.org/pdf/2005.14165.pdf table 2.1, pg. 8
block_size: 2048
lr: 0.00025
n_embed: 768
n_heads: 12
n_layers: 12
p_dropout: 0.2
tokenizer: "gpt2"
vocab_size: 50257